#!/usr/bin/env bash
set -euo pipefail
cb='
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ------ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù ------ ‚îÇ The Ultimate ‚îÇ
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù  ------ ‚îÇ Claude Code  ‚îÇ
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó  ------ ‚îÇ  Docker Dev  ‚îÇ
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó ------ ‚îÇ Environment  ‚îÇ
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ------ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
'
# Configuration
IMAGE_NAME="claudebox"
DOCKER_USER="claude"
USER_ID=$(id -u)
GROUP_ID=$(id -g)
PROJECT_DIR="$(pwd)"
SCRIPT_PATH="$(realpath "$0")"
CLAUDE_DATA_DIR="$HOME/.claude"
LINK_TARGET="/usr/local/bin/claudebox"
DOCKERFILE="$(mktemp /tmp/claudebox-dockerfile.XXXXXX)"
NODE_VERSION="--lts"
USER_ARGS=("$@")
USER_ARGS+=("--dangerously-skip-permissions")
#USER_ARGS+=("--dangerously-enable-sudo")
USER_ARGS+=("--dangerously-disable-firewall")

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
NC='\033[0m' # No Color

check_docker() {
    if ! command -v docker &> /dev/null; then
        return 1  # Docker not installed
    fi

    # Check if docker daemon is running
    if ! docker info &> /dev/null; then
        return 2  # Docker installed but not running
    fi

    # Check if user can run docker without sudo
    if ! docker ps &> /dev/null; then
        return 3  # Docker requires sudo
    fi

    return 0  # Docker is working perfectly
}

# Function to install Docker
install_docker() {
    echo -e "${YELLOW}Docker is not installed.${NC}"
    echo -e "${CYAN}Would you like to install Docker now? (y/n)${NC}"
    read -r response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        echo -e "${RED}Docker is required for ClaudeBox. Please install Docker and try again.${NC}"
        echo -e "${YELLOW}Visit: https://docs.docker.com/engine/install/${NC}"
        exit 1
    fi

    echo -e "${BLUE}Installing Docker...${NC}"

    # Detect OS
    if [[ -f /etc/os-release ]]; then
        . /etc/os-release
        OS=$ID
    else
        echo -e "${RED}Cannot detect OS. Please install Docker manually.${NC}"
        exit 1
    fi

    case $OS in
        ubuntu|debian)
            echo -e "${YELLOW}Installing Docker requires sudo privileges...${NC}"

           # Install prerequisites
            sudo apt-get update
            sudo apt-get install -y \
                ca-certificates \
                curl \
                gnupg \
                lsb-release

            # Add Docker's official GPG key
            sudo mkdir -p /etc/apt/keyrings
            curl -fsSL https://download.docker.com/linux/$OS/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

            # Set up the repository
            echo \
                "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$OS \
                $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

            # Install Docker Engine
            sudo apt-get update
            sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            ;;

        fedora|rhel|centos)
            echo -e "${YELLOW}Installing Docker requires sudo privileges...${NC}"

            # Install Docker
            sudo dnf -y install dnf-plugins-core
            sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
            sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

            # Start Docker
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;

        arch|manjaro)
            echo -e "${YELLOW}Installing Docker requires sudo privileges...${NC}"
            sudo pacman -S --noconfirm docker
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;

        *)
            echo -e "${RED}Unsupported OS: $OS${NC}"
            echo -e "${YELLOW}Please install Docker manually and run this script again.${NC}"
            echo -e "${YELLOW}Visit: https://docs.docker.com/engine/install/${NC}"
            exit 1
            ;;
    esac

    echo -e "${GREEN}Docker installed successfully!${NC}"

    # Now configure for non-root usage
    configure_docker_nonroot
}

# Function to configure Docker for non-root usage
configure_docker_nonroot() {
    echo -e "${YELLOW}Configuring Docker for non-root usage...${NC}"
    echo -e "${YELLOW}This requires sudo to add you to the docker group...${NC}"

    # Create docker group if it doesn't exist
    if ! getent group docker > /dev/null; then
        sudo groupadd docker
    fi

    # Add current user to docker group
    sudo usermod -aG docker "$USER"

    echo -e "${GREEN}Docker configured for non-root usage!${NC}"
    echo -e "${YELLOW}You need to log out and back in for group changes to take effect.${NC}"
    echo -e "${YELLOW}Or run: ${CYAN}newgrp docker${NC}"
    echo -e "${YELLOW}Then run 'claudebox' again.${NC}"

    # Try to activate the group in current shell
    echo -e "${BLUE}Trying to activate docker group in current shell...${NC}"
    exec newgrp docker
}

# Logo drawing function
logo() {
    while IFS= read -r l; do
        o="" c=""
        for ((i=0;i<${#l};i++)); do
            ch="${l:$i:1}"
            [[ "$ch" == " " ]] && { o+="$ch"; continue; }
            cc=$(printf '%d' "'$ch" 2>/dev/null||echo 0)
            if [[ $cc -ge 32 && $cc -le 126 ]]; then n='\033[33m'
            elif [[ $cc -ge 9552 && $cc -le 9580 ]]; then n='\033[34m'
            elif [[ $cc -eq 9608 ]]; then n='\033[31m'
            else n='\033[37m'; fi
            [[ "$n" != "$c" ]] && { o+="$n"; c="$n"; }
            o+="$ch"
        done
        echo -e "${o}\033[0m"
    done <<< "$cb"
}


# Check Docker installation and permissions
docker_status=$(check_docker; echo $?)

case $docker_status in
    1)
        # Docker not installed - need to install
        install_docker
        ;;
    2)
        # Docker installed but not running - need sudo to start it
        echo -e "${YELLOW}Docker is installed but not running.${NC}"
        echo -e "${YELLOW}Starting Docker requires sudo privileges...${NC}"
        sudo systemctl start docker
        if ! docker info &> /dev/null; then
            echo -e "${RED}Failed to start Docker. Please check your Docker installation.${NC}"
            exit 1
        fi

        # Check if we need to configure non-root access
        if ! docker ps &> /dev/null; then
            configure_docker_nonroot
        fi
        ;;
    3)
        # Docker requires sudo - configure non-root access
        echo -e "${YELLOW}Docker requires sudo. Setting up non-root access...${NC}"
        configure_docker_nonroot
        ;;
esac

# Profile definitions - THE MEGA COLLECTION!
declare -A PROFILES
declare -A PROFILE_DESCRIPTIONS

# C/C++ Development
PROFILES[c]="build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev"
PROFILE_DESCRIPTIONS[c]="C/C++ Development (compilers, debuggers, analyzers, build tools, cmocka, coverage, ncurses)"

# OpenWRT Development
PROFILES[openwrt]="build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils"
PROFILE_DESCRIPTIONS[openwrt]="OpenWRT Development (cross-compilation, QEMU, build essentials)"

# Rust Development
PROFILES[rust]="curl build-essential pkg-config libssl-dev"
PROFILE_DESCRIPTIONS[rust]="Rust Development (cargo and rustc will be installed separately)"

# Python Development
PROFILES[python]="python3 python3-pip python3-venv python3-dev build-essential libffi-dev libssl-dev python3-setuptools python3-wheel ipython3 black pylint mypy"
PROFILE_DESCRIPTIONS[python]="Python Development (Python 3, pip, venv, linters, formatters)"

# Go Development
PROFILES[go]="wget git build-essential"
PROFILE_DESCRIPTIONS[go]="Go Development (Go will be installed separately)"

# Node.js/JavaScript Development
PROFILES[javascript]="build-essential python3"
PROFILE_DESCRIPTIONS[javascript]="JavaScript/TypeScript Development (Node.js, npm, yarn)"

# Java Development
PROFILES[java]="openjdk-17-jdk maven gradle ant"
PROFILE_DESCRIPTIONS[java]="Java Development (OpenJDK 17, Maven, Gradle, Ant)"

# Ruby Development
PROFILES[ruby]="ruby-full ruby-dev build-essential zlib1g-dev libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev"
PROFILE_DESCRIPTIONS[ruby]="Ruby Development (Ruby, gems, build tools)"

# PHP Development
PROFILES[php]="php php-cli php-fpm php-mysql php-pgsql php-sqlite3 php-curl php-gd php-mbstring php-xml php-zip composer"
PROFILE_DESCRIPTIONS[php]="PHP Development (PHP, Composer, common extensions)"

# Database Tools
PROFILES[database]="postgresql-client mysql-client sqlite3 redis-tools mongodb-clients"
PROFILE_DESCRIPTIONS[database]="Database Tools (PostgreSQL, MySQL, SQLite, Redis, MongoDB clients)"

# DevOps Tools
PROFILES[devops]="docker.io docker-compose kubectl helm terraform ansible awscli"
PROFILE_DESCRIPTIONS[devops]="DevOps Tools (Docker, K8s, Terraform, Ansible, AWS CLI)"

# Web Development
PROFILES[web]="nginx apache2-utils curl wget httpie jq"
PROFILE_DESCRIPTIONS[web]="Web Development Tools (nginx, curl, httpie, jq)"

# Embedded Development
PROFILES[embedded]="gcc-arm-none-eabi gdb-multiarch openocd picocom minicom screen platformio"
PROFILE_DESCRIPTIONS[embedded]="Embedded Development (ARM toolchain, debuggers, serial tools)"

# Data Science
PROFILES[datascience]="python3-numpy python3-pandas python3-scipy python3-matplotlib python3-seaborn jupyter-notebook r-base"
PROFILE_DESCRIPTIONS[datascience]="Data Science (NumPy, Pandas, Jupyter, R)"

# Security Tools
PROFILES[security]="nmap tcpdump wireshark-common netcat-openbsd john hashcat hydra"
PROFILE_DESCRIPTIONS[security]="Security Tools (network analysis, penetration testing)"

# ML/AI Development
PROFILES[ml]="python3-pip python3-dev python3-venv build-essential cmake"
PROFILE_DESCRIPTIONS[ml]="Machine Learning (base tools, Python libs installed separately)"

# Spinner function
show_spinner() {
	local pid=$1
	local msg=$2
	local spin='‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è'
	local i=0
	echo -n "$msg "
	while kill -0 $pid 2>/dev/null; do
		printf "\b%s" "${spin:i++%${#spin}:1}"
		sleep 0.1
	done
	echo -e "\b${GREEN}‚úì${NC}"
}

if [[ "${1:-}" == "profile" ]]; then
    shift
    if [[ $# -eq 0 ]]; then
        echo -e "${CYAN}Available Profiles:${NC}"
        echo
        for profile in $(printf '%s\n' "${!PROFILE_DESCRIPTIONS[@]}" | sort); do
            echo -e "  ${GREEN}$profile${NC} - ${PROFILE_DESCRIPTIONS[$profile]}"
        done
        echo
        echo -e "${YELLOW}Usage: claudebox profile <name> [<name2> ...]${NC}"
        echo -e "${YELLOW}Example: claudebox profile c python web${NC}"
        exit 0
    fi

    # Install selected profiles
    PACKAGES_TO_INSTALL=""
    SELECTED_PROFILES=()

    for profile in "$@"; do
        if [[ -n "${PROFILES[$profile]:-}" ]]; then
            PACKAGES_TO_INSTALL="$PACKAGES_TO_INSTALL ${PROFILES[$profile]}"
            SELECTED_PROFILES+=("$profile")
        else
            echo -e "${RED}Unknown profile: $profile${NC}"
            echo -e "${YELLOW}Run 'claudebox profile' to see available profiles${NC}"
            exit 1
        fi
    done

    echo -e "${PURPLE}Installing profiles: ${SELECTED_PROFILES[*]}${NC}"

    # Create temporary container
    TEMP_CONTAINER=$(docker create "$IMAGE_NAME" sleep infinity)
    docker start "$TEMP_CONTAINER" >/dev/null

    # Update package lists
    docker exec -u root "$TEMP_CONTAINER" apt-get update -qq >/dev/null 2>&1 &
    show_spinner $! "Updating package lists..."

    # Install packages
    docker exec -u root "$TEMP_CONTAINER" bash -c "
        export DEBIAN_FRONTEND=noninteractive
        apt-get install -y -qq $PACKAGES_TO_INSTALL >/dev/null 2>&1
    " &
    show_spinner $! "Installing packages..."

    # Special handling for language-specific tools
    for profile in "${SELECTED_PROFILES[@]}"; do
        case "$profile" in
            rust)
                docker exec -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
                    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y >/dev/null 2>&1
                    source \$HOME/.cargo/env
                    rustup component add clippy rustfmt rust-analyzer >/dev/null 2>&1
                " &
                show_spinner $! "Installing Rust toolchain..."
                ;;
            go)
                docker exec -u root "$TEMP_CONTAINER" bash -c "
                    ARCH=\$(dpkg --print-architecture)
                    case \$ARCH in
                        amd64) GOARCH=amd64 ;;
                        arm64|aarch64) GOARCH=arm64 ;;
                        armhf) GOARCH=armv6l ;;
                        *) echo 'Unsupported architecture for Go'; exit 1 ;;
                    esac
                    wget -q https://go.dev/dl/go1.21.5.linux-\${GOARCH}.tar.gz
                    tar -C /usr/local -xzf go1.21.5.linux-\${GOARCH}.tar.gz
                    rm go1.21.5.linux-\${GOARCH}.tar.gz
                    echo 'export PATH=/usr/local/go/bin:\$PATH' >> /etc/profile
                " >/dev/null 2>&1 &
                show_spinner $! "Installing Go..."
                ;;
            javascript)
                docker exec -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
                    source \$HOME/.nvm/nvm.sh
                    npm install -g typescript ts-node eslint prettier webpack vite nodemon pm2 yarn pnpm >/dev/null 2>&1
                " &
                show_spinner $! "Installing Node.js tools..."
                ;;
            python)
                docker exec -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
                    pip3 install --user pipenv poetry virtualenv pytest requests numpy pandas matplotlib jupyterlab >/dev/null 2>&1
                " &
                show_spinner $! "Installing Python tools..."
                ;;
            ml)
                docker exec -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
                    pip3 install --user torch torchvision torchaudio tensorflow scikit-learn transformers datasets >/dev/null 2>&1
                " &
                show_spinner $! "Installing ML libraries..."
                ;;
        esac
    done

    # Clean apt cache
    docker exec -u root "$TEMP_CONTAINER" bash -c "
        apt-get clean >/dev/null 2>&1
        rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
    " >/dev/null 2>&1

    # Commit changes
    docker commit "$TEMP_CONTAINER" "$IMAGE_NAME" >/dev/null &
    show_spinner $! "Saving changes to image..."

    # Cleanup
    docker rm -f "$TEMP_CONTAINER" >/dev/null

    echo -e "${GREEN}Profile installation complete!${NC}"
    exit 0
fi
# Other commands (install, shell, etc.)
if [[ "${1:-}" == "install" ]]; then
    shift
    PACKAGES_TO_INSTALL=("$@")
    if [[ ${#PACKAGES_TO_INSTALL[@]} -eq 0 ]]; then
        echo "Error: No packages specified. Usage: claudebox install <package1> <package2> ..."
        exit 1
    fi

    echo "Installing additional packages: ${PACKAGES_TO_INSTALL[*]}"

    TEMP_CONTAINER=$(docker create "$IMAGE_NAME" sleep infinity)
    docker start "$TEMP_CONTAINER" >/dev/null

    docker exec -u root "$TEMP_CONTAINER" bash -c "
        export DEBIAN_FRONTEND=noninteractive
        apt-get update -qq
        apt-get install -y -qq ${PACKAGES_TO_INSTALL[*]} 2>&1 | grep -v 'invoke-rc.d' | grep -v 'policy-rc.d' || true
        apt-get clean
        rm -rf /var/lib/apt/lists/*
    "

    docker commit "$TEMP_CONTAINER" "$IMAGE_NAME" >/dev/null
    docker rm -f "$TEMP_CONTAINER" >/dev/null

    echo "Packages installed successfully!"
    exit 0
fi

if [[ "${1:-}" == "update" ]]; then
    echo -e "${BLUE}Updating Claude code...${NC}"

    # Create temporary container
    TEMP_CONTAINER=$(docker create "$IMAGE_NAME" sleep infinity)
    docker start "$TEMP_CONTAINER" >/dev/null

    # Step 1: Run claude update to download the update
    docker exec -it -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
        source \$HOME/.nvm/nvm.sh
        nvm use default
        claude update
    "

    # Step 2: Run claude again to apply the update (simulating a restart)
    echo -e "${BLUE}Applying update...${NC}"
    docker exec -it -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
        source \$HOME/.nvm/nvm.sh
        nvm use default
        # Run claude --version to trigger the update application
        claude --version
    "
    # Commit changes back to the image
    docker commit "$TEMP_CONTAINER" "$IMAGE_NAME" >/dev/null
    docker rm -f "$TEMP_CONTAINER" >/dev/null

    echo -e "${GREEN}Claude code update completed and saved to image!${NC}"
    exit 0
fi

# Commands that need to persist changes to the image
PERSISTENT_COMMANDS="config|mcp|migrate-installer|update"

if [[ "${1:-}" =~ ^($PERSISTENT_COMMANDS)$ ]]; then
    # Ensure image exists
    if ! docker image inspect "$IMAGE_NAME" &>/dev/null; then
        echo -e "${RED}Error: ClaudeBox image not found.${NC}"
        echo -e "Run ${GREEN}claudebox${NC} first to build the image."
        exit 1
    fi

    if [[ "${1:-}" == "update" ]]; then
        TEMP_CONTAINER=$(docker create "$IMAGE_NAME" sleep infinity)
        docker start "$TEMP_CONTAINER" >/dev/null

        docker exec -it -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
            source \$HOME/.nvm/nvm.sh
            nvm use default
            claude update"

        docker exec -it -u "$DOCKER_USER" "$TEMP_CONTAINER" bash -c "
            source \$HOME/.nvm/nvm.sh
            nvm use default
            claude --version"

        docker commit "$TEMP_CONTAINER" "$IMAGE_NAME" >/dev/null
        docker rm -f "$TEMP_CONTAINER" >/dev/null
    else
        # Other commands - just run them
        TEMP_CONTAINER=$(docker create "$IMAGE_NAME" sleep infinity)
        docker start "$TEMP_CONTAINER" >/dev/null

        docker exec -it -u "$DOCKER_USER" "$TEMP_CONTAINER" \
            /home/$DOCKER_USER/claude-wrapper "$@"
        docker commit "$TEMP_CONTAINER" "$IMAGE_NAME" >/dev/null
        docker rm -f "$TEMP_CONTAINER" >/dev/null
    fi
    exit 0
fi


if [[ "${1:-}" == "shell" ]]; then
    docker run -it --rm \
        -u "$DOCKER_USER" \
        -w /workspace \
        -v "$PROJECT_DIR":/workspace \
        -v "$CLAUDE_DATA_DIR":/home/$DOCKER_USER/.claude \
        -v "$HOME/.claudebox":/home/$DOCKER_USER/.claudebox \
        -v "$HOME/.config/claude":/home/$DOCKER_USER/.config/claude \
        -v "$HOME/.claude.json":/home/$DOCKER_USER/.claude.json \
        -v "$HOME/.npmrc":/home/$DOCKER_USER/.npmrc:ro \
        -v "$HOME/.ssh":/home/$DOCKER_USER/.ssh:ro \
        -e "NODE_ENV=${NODE_ENV:-production}" \
        -e "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}" \
        --cap-add NET_ADMIN \
        --cap-add NET_RAW \
        --entrypoint /bin/bash \
        "$IMAGE_NAME"
    exit 0
fi

if [[ "${1:-}" == "clean" ]]; then
    shift
    if [[ "${1:-}" == "--all" || "${1:-}" == "-a" ]]; then
        echo -e "${YELLOW}Deep cleaning ClaudeBox and build cache...${NC}"
        docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
        docker builder prune -af  # -a removes ALL build cache
        echo -e "${GREEN}Deep clean complete!${NC}"
    else
        echo -e "${YELLOW}Cleaning ClaudeBox...${NC}"
        docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
        docker builder prune -f   # Only dangling build cache
        echo -e "${GREEN}Clean complete!${NC}"
    fi
    docker system df
    exit 0
fi

if [[ "${1:-}" == "help" || "${1:-}" == "--help" || "${1:-}" == "-h" ]]; then
    # --help or -h -> show Claude's help
    if docker image inspect "$IMAGE_NAME" &>/dev/null; then
        docker run --rm \
            -u "$DOCKER_USER" \
            --entrypoint /home/$DOCKER_USER/claude-wrapper \
            "$IMAGE_NAME" --help | sed '1s/claude/claudebox/g'
        echo
        echo -e "${WHITE}Added Options:${NC}"
        echo -e "${CYAN}  --dangerously-enable-sudo       ${WHITE}Enable sudo without password"
        echo -e "${CYAN}  --dangerously-disable-firewall  ${WHITE}Disable network restrictions"
        echo
        echo -e "${WHITE}Added Commands:"
        echo -e "  profile [names...]              Install language profiles"
        echo -e "  install <packages>              Install apt packages"
        echo -e "  shell                           Open bash shell in container"
        echo -e "  info                            Show ClaudeBox information"
        echo -e "  clean                           Remove ClaudeBox image"
        echo -e "  rebuild                         Rebuild Docker image${NC}"
    else
        echo -e "${CYAN}ClaudeBox - Claude Code Docker Environment${NC}"
        echo
        echo -e "${YELLOW}First run setup required!${NC}"
        echo "Run script without arguments first to build the Docker image."
        echo
    fi
    exit 0
fi

if [[ "${1:-}" == "rebuild" ]]; then
    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true &
    show_spinner $! "Wiping ClaudeBox Image Data..."
fi

mkdir -p "$CLAUDE_DATA_DIR"
mkdir -p "$HOME/.claudebox"

# Handle MCP config in workspace
MCP_BACKUP=""
EXPECTED_THINKING="/home/$DOCKER_USER/mcp-servers/run-thinking.sh"
EXPECTED_MEMORY="/home/$DOCKER_USER/mcp-servers/run-memory.sh"

if [[ -f "$PROJECT_DIR/.mcp.json" ]]; then
    # Check if it's already configured correctly
    THINKING_CMD=$(jq -r '.mcpServers.thinking.command // ""' "$PROJECT_DIR/.mcp.json" 2>/dev/null || echo "")
    MEMORY_CMD=$(jq -r '.mcpServers.memory.command // ""' "$PROJECT_DIR/.mcp.json" 2>/dev/null || echo "")
    
    if [[ "$THINKING_CMD" != "$EXPECTED_THINKING" ]] || [[ "$MEMORY_CMD" != "$EXPECTED_MEMORY" ]]; then
        # Different config - back it up and replace
        MCP_BACKUP="$PROJECT_DIR/.mcp.json.claudebox-backup"
        cp "$PROJECT_DIR/.mcp.json" "$MCP_BACKUP"
        echo -e "${BLUE}Backed up existing .mcp.json${NC}"
        
        # Create our MCP config
        cat > "$PROJECT_DIR/.mcp.json" << EOF
{
  "mcpServers": {
    "thinking": {
      "command": "/home/$DOCKER_USER/mcp-servers/run-thinking.sh",
      "args": []
    },
    "memory": {
      "command": "/home/$DOCKER_USER/mcp-servers/run-memory.sh",
      "args": [],
      "env": {
        "MEMORY_FILE_PATH": "/home/$DOCKER_USER/.claudebox/memory.json"
      }
    }
  }
}
EOF
        
        # Set up trap to restore on exit
        trap 'mv "$MCP_BACKUP" "$PROJECT_DIR/.mcp.json" && echo -e "${BLUE}Restored original .mcp.json${NC}"' EXIT
    fi
    # else: already correct, do nothing
else
    # No file exists - create it
    cat > "$PROJECT_DIR/.mcp.json" << EOF
{
  "mcpServers": {
    "thinking": {
      "command": "/home/$DOCKER_USER/mcp-servers/run-thinking.sh",
      "args": []
    },
    "memory": {
      "command": "/home/$DOCKER_USER/mcp-servers/run-memory.sh",
      "args": [],
      "env": {
        "MEMORY_FILE_PATH": "/home/$DOCKER_USER/.claudebox/memory.json"
      }
    }
  }
}
EOF
    echo -e "${GREEN}Created .mcp.json with ClaudeBox MCP servers${NC}"
fi

# Check if image exists
first_time=false
if ! docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
    logo

cat > "$DOCKERFILE" <<'EOF'
FROM debian:bookworm

ARG USER_ID
ARG GROUP_ID
ARG USERNAME
ARG NODE_VERSION

# Prevent service startup in Docker
RUN echo '#!/bin/sh\nexit 101' > /usr/sbin/policy-rc.d && \
    chmod +x /usr/sbin/policy-rc.d

# Install base dependencies
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq \
    curl gnupg ca-certificates sudo git iptables ipset \
    && apt-get clean

# Create user with matching UID/GID
RUN groupadd -g $GROUP_ID $USERNAME || true \
    && useradd -m -u $USER_ID -g $GROUP_ID -s /bin/bash $USERNAME

# Install basic development tools
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq \
    build-essential git wget curl unzip file vim nano \
    jq make less rsync openssh-client \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Switch to user for NVM installation
USER $USERNAME
WORKDIR /home/$USERNAME

# Install NVM
ENV NVM_DIR="/home/$USERNAME/.nvm"
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash

RUN bash -c "source $NVM_DIR/nvm.sh && \
    if [[ \"$NODE_VERSION\" == '--lts' ]]; then \
        nvm install --lts && \
        nvm alias default 'lts/*'; \
    else \
        nvm install $NODE_VERSION && \
        nvm alias default $NODE_VERSION; \
    fi && \
    nvm use default"

# Add NVM to bashrc for persistence
RUN echo 'export NVM_DIR="$HOME/.nvm"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' >> ~/.bashrc

# Install Claude CLI using NVM's node
RUN bash -c "source $NVM_DIR/nvm.sh && \
    nvm use default && \
    npm install -g @anthropic-ai/claude-code"

# Install MCP SDK dependencies
RUN bash -c "source $NVM_DIR/nvm.sh && \
    nvm use default && \
    npm install @modelcontextprotocol/sdk chalk"

# Create proper MCP server packages with TypeScript compilation
RUN mkdir -p ~/mcp-servers/thinking ~/mcp-servers/memory

# Create package.json for thinking server
RUN cat > ~/mcp-servers/thinking/package.json << 'PACKAGEEOF'
{
  "name": "@modelcontextprotocol/server-sequential-thinking",
  "version": "0.6.2",
  "description": "MCP server for sequential thinking and problem solving",
  "license": "MIT",
  "type": "module",
  "bin": {
    "mcp-server-sequential-thinking": "dist/index.js"
  },
  "files": ["dist"],
  "scripts": {
    "build": "tsc && chmod +x dist/*.js",
    "prepare": "npm run build"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "0.5.0",
    "chalk": "^5.3.0"
  },
  "devDependencies": {
    "@types/node": "^22",
    "typescript": "^5.3.3"
  }
}
PACKAGEEOF

# Create tsconfig.json for thinking server
RUN cat > ~/mcp-servers/thinking/tsconfig.json << 'TSCONFIGEOF'
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "outDir": "./dist",
    "rootDir": ".",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["./**/*.ts"]
}
TSCONFIGEOF

# Create the TypeScript source file for thinking server
RUN cat > ~/mcp-servers/thinking/index.ts << 'THINKEOF'
#!/usr/bin/env node

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
  Tool,
} from "@modelcontextprotocol/sdk/types.js";
import chalk from 'chalk';

interface ThoughtData {
  thought: string;
  thoughtNumber: number;
  totalThoughts: number;
  isRevision?: boolean;
  revisesThought?: number;
  branchFromThought?: number;
  branchId?: string;
  needsMoreThoughts?: boolean;
  nextThoughtNeeded: boolean;
}

class SequentialThinkingServer {
  private thoughtHistory: ThoughtData[] = [];
  private branches: Record<string, ThoughtData[]> = {};

  private validateThoughtData(input: unknown): ThoughtData {
    const data = input as Record<string, unknown>;

    if (!data.thought || typeof data.thought !== 'string') {
      throw new Error('Invalid thought: must be a string');
    }
    if (!data.thoughtNumber || typeof data.thoughtNumber !== 'number') {
      throw new Error('Invalid thoughtNumber: must be a number');
    }
    if (!data.totalThoughts || typeof data.totalThoughts !== 'number') {
      throw new Error('Invalid totalThoughts: must be a number');
    }
    if (typeof data.nextThoughtNeeded !== 'boolean') {
      throw new Error('Invalid nextThoughtNeeded: must be a boolean');
    }

    return {
      thought: data.thought,
      thoughtNumber: data.thoughtNumber,
      totalThoughts: data.totalThoughts,
      nextThoughtNeeded: data.nextThoughtNeeded,
      isRevision: data.isRevision as boolean | undefined,
      revisesThought: data.revisesThought as number | undefined,
      branchFromThought: data.branchFromThought as number | undefined,
      branchId: data.branchId as string | undefined,
      needsMoreThoughts: data.needsMoreThoughts as boolean | undefined,
    };
  }

  private formatThought(thoughtData: ThoughtData): string {
    const { thoughtNumber, totalThoughts, thought, isRevision, revisesThought, branchFromThought, branchId } = thoughtData;

    let prefix = '';
    let context = '';

    if (isRevision) {
      prefix = chalk.yellow('üîÑ Revision');
      context = ` (revising thought ${revisesThought})`;
    } else if (branchFromThought) {
      prefix = chalk.green('üåø Branch');
      context = ` (from thought ${branchFromThought}, ID: ${branchId})`;
    } else {
      prefix = chalk.blue('üí≠ Thought');
      context = '';
    }

    const header = `${prefix} ${thoughtNumber}/${totalThoughts}${context}`;
    const border = '‚îÄ'.repeat(Math.max(header.length, thought.length) + 4);

    return `
‚îå${border}‚îê
‚îÇ ${header} ‚îÇ
‚îú${border}‚î§
‚îÇ ${thought.padEnd(border.length - 2)} ‚îÇ
‚îî${border}‚îò`;
  }

  public processThought(input: unknown): { content: Array<{ type: string; text: string }>; isError?: boolean } {
    try {
      const validatedInput = this.validateThoughtData(input);

      if (validatedInput.thoughtNumber > validatedInput.totalThoughts) {
        validatedInput.totalThoughts = validatedInput.thoughtNumber;
      }

      this.thoughtHistory.push(validatedInput);

      if (validatedInput.branchFromThought && validatedInput.branchId) {
        if (!this.branches[validatedInput.branchId]) {
          this.branches[validatedInput.branchId] = [];
        }
        this.branches[validatedInput.branchId].push(validatedInput);
      }

      const formattedThought = this.formatThought(validatedInput);
      console.error(formattedThought);

      return {
        content: [{
          type: "text",
          text: JSON.stringify({
            thoughtNumber: validatedInput.thoughtNumber,
            totalThoughts: validatedInput.totalThoughts,
            nextThoughtNeeded: validatedInput.nextThoughtNeeded,
            branches: Object.keys(this.branches),
            thoughtHistoryLength: this.thoughtHistory.length
          }, null, 2)
        }]
      };
    } catch (error) {
      return {
        content: [{
          type: "text",
          text: JSON.stringify({
            error: error instanceof Error ? error.message : String(error),
            status: 'failed'
          }, null, 2)
        }],
        isError: true
      };
    }
  }
}

const SEQUENTIAL_THINKING_TOOL: Tool = {
  name: "sequentialthinking",
  description: `A detailed tool for dynamic and reflective problem-solving through thoughts.
This tool helps analyze problems through a flexible thinking process that can adapt and evolve.
Each thought can build on, question, or revise previous insights as understanding deepens.

When to use this tool:
- Breaking down complex problems into steps
- Planning and design with room for revision
- Analysis that might need course correction
- Problems where the full scope might not be clear initially
- Problems that require a multi-step solution
- Tasks that need to maintain context over multiple steps
- Situations where irrelevant information needs to be filtered out

Key features:
- You can adjust total_thoughts up or down as you progress
- You can question or revise previous thoughts
- You can add more thoughts even after reaching what seemed like the end
- You can express uncertainty and explore alternative approaches
- Not every thought needs to build linearly - you can branch or backtrack
- Generates a solution hypothesis
- Verifies the hypothesis based on the Chain of Thought steps
- Repeats the process until satisfied
- Provides a correct answer

Parameters explained:
- thought: Your current thinking step, which can include:
  * Regular analytical steps
  * Revisions of previous thoughts
  * Questions about previous decisions
  * Realizations about needing more analysis
  * Changes in approach
  * Hypothesis generation
  * Hypothesis verification
- next_thought_needed: True if you need more thinking, even if at what seemed like the end
- thought_number: Current number in sequence (can go beyond initial total if needed)
- total_thoughts: Current estimate of thoughts needed (can be adjusted up/down)
- is_revision: A boolean indicating if this thought revises previous thinking
- revises_thought: If is_revision is true, which thought number is being reconsidered
- branch_from_thought: If branching, which thought number is the branching point
- branch_id: Identifier for the current branch (if any)
- needs_more_thoughts: If reaching end but realizing more thoughts needed

You should:
1. Start with an initial estimate of needed thoughts, but be ready to adjust
2. Feel free to question or revise previous thoughts
3. Don't hesitate to add more thoughts if needed, even at the "end"
4. Express uncertainty when present
5. Mark thoughts that revise previous thinking or branch into new paths
6. Ignore information that is irrelevant to the current step
7. Generate a solution hypothesis when appropriate
8. Verify the hypothesis based on the Chain of Thought steps
9. Repeat the process until satisfied with the solution
10. Provide a single, ideally correct answer as the final output
11. Only set next_thought_needed to false when truly done and a satisfactory answer is reached`,
  inputSchema: {
    type: "object",
    properties: {
      thought: {
        type: "string",
        description: "Your current thinking step"
      },
      nextThoughtNeeded: {
        type: "boolean",
        description: "Whether another thought step is needed"
      },
      thoughtNumber: {
        type: "integer",
        description: "Current thought number",
        minimum: 1
      },
      totalThoughts: {
        type: "integer",
        description: "Estimated total thoughts needed",
        minimum: 1
      },
      isRevision: {
        type: "boolean",
        description: "Whether this revises previous thinking"
      },
      revisesThought: {
        type: "integer",
        description: "Which thought is being reconsidered",
        minimum: 1
      },
      branchFromThought: {
        type: "integer",
        description: "Branching point thought number",
        minimum: 1
      },
      branchId: {
        type: "string",
        description: "Branch identifier"
      },
      needsMoreThoughts: {
        type: "boolean",
        description: "If more thoughts are needed"
      }
    },
    required: ["thought", "nextThoughtNeeded", "thoughtNumber", "totalThoughts"]
  }
};

const server = new Server(
  {
    name: "sequential-thinking-server",
    version: "0.2.0",
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

const thinkingServer = new SequentialThinkingServer();

server.setRequestHandler(ListToolsRequestSchema, async () => ({
  tools: [SEQUENTIAL_THINKING_TOOL],
}));

server.setRequestHandler(CallToolRequestSchema, async (request) => {
  if (request.params.name === "sequentialthinking") {
    return thinkingServer.processThought(request.params.arguments);
  }

  return {
    content: [{
      type: "text",
      text: `Unknown tool: ${request.params.name}`
    }],
    isError: true
  };
});

async function runServer() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error("Sequential Thinking MCP Server running on stdio");
}

runServer().catch((error) => {
  console.error("Fatal error running server:", error);
  process.exit(1);
});
THINKEOF

# Create package.json for memory server
RUN cat > ~/mcp-servers/memory/package.json << 'PACKAGEEOF'
{
  "name": "@modelcontextprotocol/server-memory",
  "version": "0.6.3",
  "description": "MCP server for enabling memory for Claude through a knowledge graph",
  "license": "MIT",
  "type": "module",
  "bin": {
    "mcp-server-memory": "dist/index.js"
  },
  "files": ["dist"],
  "scripts": {
    "build": "tsc && chmod +x dist/*.js",
    "prepare": "npm run build"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "1.0.1"
  },
  "devDependencies": {
    "@types/node": "^22",
    "typescript": "^5.6.2"
  }
}
PACKAGEEOF

# Create tsconfig.json for memory server
RUN cat > ~/mcp-servers/memory/tsconfig.json << 'TSCONFIGEOF'
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "outDir": "./dist",
    "rootDir": ".",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["./**/*.ts"]
}
TSCONFIGEOF

# Create the TypeScript source file for memory server
RUN cat > ~/mcp-servers/memory/index.ts << 'MEMORYEOF'
#!/usr/bin/env node

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import { promises as fs } from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const defaultMemoryPath = path.join(path.dirname(fileURLToPath(import.meta.url)), 'memory.json');

const MEMORY_FILE_PATH = process.env.MEMORY_FILE_PATH
  ? path.isAbsolute(process.env.MEMORY_FILE_PATH)
    ? process.env.MEMORY_FILE_PATH
    : path.join(path.dirname(fileURLToPath(import.meta.url)), process.env.MEMORY_FILE_PATH)
  : defaultMemoryPath;

interface Entity {
  name: string;
  entityType: string;
  observations: string[];
}

interface Relation {
  from: string;
  to: string;
  relationType: string;
}

interface KnowledgeGraph {
  entities: Entity[];
  relations: Relation[];
}

class KnowledgeGraphManager {
  private async loadGraph(): Promise<KnowledgeGraph> {
    try {
      const data = await fs.readFile(MEMORY_FILE_PATH, "utf-8");
      const lines = data.split("\n").filter(line => line.trim() !== "");
      return lines.reduce((graph: KnowledgeGraph, line) => {
        const item = JSON.parse(line);
        if (item.type === "entity") graph.entities.push(item as Entity);
        if (item.type === "relation") graph.relations.push(item as Relation);
        return graph;
      }, { entities: [], relations: [] });
    } catch (error) {
      if (error instanceof Error && 'code' in error && (error as any).code === "ENOENT") {
        return { entities: [], relations: [] };
      }
      throw error;
    }
  }

  private async saveGraph(graph: KnowledgeGraph): Promise<void> {
    const lines = [
      ...graph.entities.map(e => JSON.stringify({ type: "entity", ...e })),
      ...graph.relations.map(r => JSON.stringify({ type: "relation", ...r })),
    ];
    await fs.writeFile(MEMORY_FILE_PATH, lines.join("\n"));
  }

  async createEntities(entities: Entity[]): Promise<Entity[]> {
    const graph = await this.loadGraph();
    const newEntities = entities.filter(e => !graph.entities.some(existingEntity => existingEntity.name === e.name));
    graph.entities.push(...newEntities);
    await this.saveGraph(graph);
    return newEntities;
  }

  async createRelations(relations: Relation[]): Promise<Relation[]> {
    const graph = await this.loadGraph();
    const newRelations = relations.filter(r => !graph.relations.some(existingRelation =>
      existingRelation.from === r.from &&
      existingRelation.to === r.to &&
      existingRelation.relationType === r.relationType
    ));
    graph.relations.push(...newRelations);
    await this.saveGraph(graph);
    return newRelations;
  }

  async addObservations(observations: { entityName: string; contents: string[] }[]): Promise<{ entityName: string; addedObservations: string[] }[]> {
    const graph = await this.loadGraph();
    const results = observations.map(o => {
      const entity = graph.entities.find(e => e.name === o.entityName);
      if (!entity) {
        throw new Error(`Entity with name ${o.entityName} not found`);
      }
      const newObservations = o.contents.filter(content => !entity.observations.includes(content));
      entity.observations.push(...newObservations);
      return { entityName: o.entityName, addedObservations: newObservations };
    });
    await this.saveGraph(graph);
    return results;
  }

  async deleteEntities(entityNames: string[]): Promise<void> {
    const graph = await this.loadGraph();
    graph.entities = graph.entities.filter(e => !entityNames.includes(e.name));
    graph.relations = graph.relations.filter(r => !entityNames.includes(r.from) && !entityNames.includes(r.to));
    await this.saveGraph(graph);
  }

  async deleteObservations(deletions: { entityName: string; observations: string[] }[]): Promise<void> {
    const graph = await this.loadGraph();
    deletions.forEach(d => {
      const entity = graph.entities.find(e => e.name === d.entityName);
      if (entity) {
        entity.observations = entity.observations.filter(o => !d.observations.includes(o));
      }
    });
    await this.saveGraph(graph);
  }

  async deleteRelations(relations: Relation[]): Promise<void> {
    const graph = await this.loadGraph();
    graph.relations = graph.relations.filter(r => !relations.some(delRelation =>
      r.from === delRelation.from &&
      r.to === delRelation.to &&
      r.relationType === delRelation.relationType
    ));
    await this.saveGraph(graph);
  }

  async readGraph(): Promise<KnowledgeGraph> {
    return this.loadGraph();
  }

  async searchNodes(query: string): Promise<KnowledgeGraph> {
    const graph = await this.loadGraph();

    const filteredEntities = graph.entities.filter(e =>
      e.name.toLowerCase().includes(query.toLowerCase()) ||
      e.entityType.toLowerCase().includes(query.toLowerCase()) ||
      e.observations.some(o => o.toLowerCase().includes(query.toLowerCase()))
    );

    const filteredEntityNames = new Set(filteredEntities.map(e => e.name));

    const filteredRelations = graph.relations.filter(r =>
      filteredEntityNames.has(r.from) && filteredEntityNames.has(r.to)
    );

    const filteredGraph: KnowledgeGraph = {
      entities: filteredEntities,
      relations: filteredRelations,
    };

    return filteredGraph;
  }

  async openNodes(names: string[]): Promise<KnowledgeGraph> {
    const graph = await this.loadGraph();

    const filteredEntities = graph.entities.filter(e => names.includes(e.name));

    const filteredEntityNames = new Set(filteredEntities.map(e => e.name));

    const filteredRelations = graph.relations.filter(r =>
      filteredEntityNames.has(r.from) && filteredEntityNames.has(r.to)
    );

    const filteredGraph: KnowledgeGraph = {
      entities: filteredEntities,
      relations: filteredRelations,
    };

    return filteredGraph;
  }
}

const knowledgeGraphManager = new KnowledgeGraphManager();

const server = new Server({
  name: "memory-server",
  version: "0.6.3",
}, {
  capabilities: {
    tools: {},
  },
});

server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: "create_entities",
        description: "Create multiple new entities in the knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            entities: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  name: { type: "string", description: "The name of the entity" },
                  entityType: { type: "string", description: "The type of the entity" },
                  observations: {
                    type: "array",
                    items: { type: "string" },
                    description: "An array of observation contents associated with the entity"
                  },
                },
                required: ["name", "entityType", "observations"],
              },
            },
          },
          required: ["entities"],
        },
      },
      {
        name: "create_relations",
        description: "Create multiple new relations between entities in the knowledge graph. Relations should be in active voice",
        inputSchema: {
          type: "object",
          properties: {
            relations: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  from: { type: "string", description: "The name of the entity where the relation starts" },
                  to: { type: "string", description: "The name of the entity where the relation ends" },
                  relationType: { type: "string", description: "The type of the relation" },
                },
                required: ["from", "to", "relationType"],
              },
            },
          },
          required: ["relations"],
        },
      },
      {
        name: "add_observations",
        description: "Add new observations to existing entities in the knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            observations: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  entityName: { type: "string", description: "The name of the entity to add the observations to" },
                  contents: {
                    type: "array",
                    items: { type: "string" },
                    description: "An array of observation contents to add"
                  },
                },
                required: ["entityName", "contents"],
              },
            },
          },
          required: ["observations"],
        },
      },
      {
        name: "delete_entities",
        description: "Delete multiple entities and their associated relations from the knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            entityNames: {
              type: "array",
              items: { type: "string" },
              description: "An array of entity names to delete"
            },
          },
          required: ["entityNames"],
        },
      },
      {
        name: "delete_observations",
        description: "Delete specific observations from entities in the knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            deletions: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  entityName: { type: "string", description: "The name of the entity containing the observations" },
                  observations: {
                    type: "array",
                    items: { type: "string" },
                    description: "An array of observations to delete"
                  },
                },
                required: ["entityName", "observations"],
              },
            },
          },
          required: ["deletions"],
        },
      },
      {
        name: "delete_relations",
        description: "Delete multiple relations from the knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            relations: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  from: { type: "string", description: "The name of the entity where the relation starts" },
                  to: { type: "string", description: "The name of the entity where the relation ends" },
                  relationType: { type: "string", description: "The type of the relation" },
                },
                required: ["from", "to", "relationType"],
              },
              description: "An array of relations to delete"
            },
          },
          required: ["relations"],
        },
      },
      {
        name: "read_graph",
        description: "Read the entire knowledge graph",
        inputSchema: {
          type: "object",
          properties: {},
        },
      },
      {
        name: "search_nodes",
        description: "Search for nodes in the knowledge graph based on a query",
        inputSchema: {
          type: "object",
          properties: {
            query: { type: "string", description: "The search query to match against entity names, types, and observation content" },
          },
          required: ["query"],
        },
      },
      {
        name: "open_nodes",
        description: "Open specific nodes in the knowledge graph by their names",
        inputSchema: {
          type: "object",
          properties: {
            names: {
              type: "array",
              items: { type: "string" },
              description: "An array of entity names to retrieve",
            },
          },
          required: ["names"],
        },
      },
    ],
  };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;

  if (!args) {
    throw new Error(`No arguments provided for tool: ${name}`);
  }

  switch (name) {
    case "create_entities":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.createEntities(args.entities as Entity[]), null, 2) }] };
    case "create_relations":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.createRelations(args.relations as Relation[]), null, 2) }] };
    case "add_observations":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.addObservations(args.observations as { entityName: string; contents: string[] }[]), null, 2) }] };
    case "delete_entities":
      await knowledgeGraphManager.deleteEntities(args.entityNames as string[]);
      return { content: [{ type: "text", text: "Entities deleted successfully" }] };
    case "delete_observations":
      await knowledgeGraphManager.deleteObservations(args.deletions as { entityName: string; observations: string[] }[]);
      return { content: [{ type: "text", text: "Observations deleted successfully" }] };
    case "delete_relations":
      await knowledgeGraphManager.deleteRelations(args.relations as Relation[]);
      return { content: [{ type: "text", text: "Relations deleted successfully" }] };
    case "read_graph":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.readGraph(), null, 2) }] };
    case "search_nodes":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.searchNodes(args.query as string), null, 2) }] };
    case "open_nodes":
      return { content: [{ type: "text", text: JSON.stringify(await knowledgeGraphManager.openNodes(args.names as string[]), null, 2) }] };
    default:
      throw new Error(`Unknown tool: ${name}`);
  }
});

async function main() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error("Knowledge Graph MCP Server running on stdio");
}

main().catch((error) => {
  console.error("Fatal error in main():", error);
  process.exit(1);
});
MEMORYEOF

RUN bash -c "source $NVM_DIR/nvm.sh && \
    cd ~/mcp-servers/thinking && \
    npm install && \
    npm run build && \
    cd ~/mcp-servers/memory && \
    npm install && \
    npm run build"

RUN cat > ~/mcp-servers/run-thinking.sh << 'WRAPPEREOF'
#!/bin/bash
echo "Starting thinking server..." >&2
echo "PATH: $PATH" >&2
echo "Node version:" >&2
node --version >&2

source $HOME/.nvm/nvm.sh
nvm use default >/dev/null 2>&1

echo "After NVM setup:" >&2
which node >&2
node --version >&2

cd $HOME/mcp-servers/thinking
exec node $HOME/mcp-servers/thinking/dist/index.js "$@"
WRAPPEREOF

RUN cat > ~/mcp-servers/run-memory.sh << 'WRAPPEREOF'
#!/bin/bash
echo "Starting memory server..." >&2
echo "PATH: $PATH" >&2
echo "Node version:" >&2
node --version >&2

source $HOME/.nvm/nvm.sh
nvm use default >/dev/null 2>&1

echo "After NVM setup:" >&2
which node >&2
node --version >&2

cd $HOME/mcp-servers/memory
exec node $HOME/mcp-servers/memory/dist/index.js "$@"
WRAPPEREOF

RUN chmod +x ~/mcp-servers/thinking/dist/*.js ~/mcp-servers/memory/dist/*.js ~/mcp-servers/run-thinking.sh ~/mcp-servers/run-memory.sh

# Add this to the Dockerfile
RUN cat > ~/test-mcp.sh << 'TESTEOF'
#!/bin/bash
echo "Testing MCP servers..."

# Test thinking server
echo "Testing thinking server..."
if timeout 2 $HOME/mcp-servers/run-thinking.sh 2>&1 | grep -q "Sequential Thinking MCP Server running on stdio"; then
    echo "‚úì Thinking server started successfully"
else
    echo "‚úó Thinking server failed to start"
fi


# Test memory server
echo "Testing memory server..."
if timeout 2 $HOME/mcp-servers/run-memory.sh 2>&1 | grep -q "Knowledge Graph MCP Server running on stdio"; then
    echo "‚úì Memory server started successfully"
else
    echo "‚úó Memory server failed to start"
fi

# Check Claude config
echo "Claude MCP config:"
cat ~/.config/claude/config.json
TESTEOF

RUN chmod +x ~/test-mcp.sh

RUN cat > ~/init-firewall.sh << 'FIREWALLEOF'
#!/bin/bash
set -euo pipefail

if [ "${DISABLE_FIREWALL:-false}" = "true" ]; then
    echo "Firewall disabled, skipping setup"
    rm -f "$0"
    exit 0
fi

iptables -F OUTPUT 2>/dev/null || true
iptables -F INPUT 2>/dev/null || true

iptables -A OUTPUT -p udp --dport 53 -j ACCEPT
iptables -A OUTPUT -p tcp --dport 53 -j ACCEPT
iptables -A INPUT -p udp --sport 53 -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT
iptables -A INPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT

iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

if command -v ipset >/dev/null 2>&1; then
    ipset destroy allowed-domains 2>/dev/null || true
    ipset create allowed-domains hash:net
    
    for domain in api.anthropic.com console.anthropic.com; do
        ips=$(getent hosts $domain | awk '{print $1}')
        for ip in $ips; do
            ipset add allowed-domains $ip 2>/dev/null || true
        done
    done
    
    iptables -A OUTPUT -m set --match-set allowed-domains dst -j ACCEPT
else
    iptables -A OUTPUT -d api.anthropic.com -j ACCEPT
    iptables -A OUTPUT -d console.anthropic.com -j ACCEPT
fi

iptables -P OUTPUT DROP
iptables -P INPUT DROP

echo "Firewall initialized with Anthropic-only access"
rm -f "$0"
FIREWALLEOF

RUN chmod +x ~/init-firewall.sh

RUN mkdir -p ~/.config/claude && \
   cat > ~/.config/claude/config.json << CONFIGEOF
{
  "mcpServers": {
    "thinking": {
      "command": "/home/$USERNAME/mcp-servers/run-thinking.sh",
      "args": []
    },
    "memory": {
      "command": "/home/$USERNAME/mcp-servers/run-memory.sh",
      "args": [],
      "env": {
        "MEMORY_FILE_PATH": "/home/$USERNAME/.claudebox/memory.json"
      }
    }
  }
}
CONFIGEOF

RUN bash -c "source $NVM_DIR/nvm.sh && claude --version"


RUN echo '#!/bin/bash' > ~/claude-wrapper && \
   echo 'export NVM_DIR="$HOME/.nvm"' >> ~/claude-wrapper && \
   echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/claude-wrapper && \
   echo '' >> ~/claude-wrapper && \
   echo '# Filter out security flags before passing to claude' >> ~/claude-wrapper && \
   echo 'FILTERED_ARGS=()' >> ~/claude-wrapper && \
   echo 'for arg in "$@"; do' >> ~/claude-wrapper && \
   echo '    case "$arg" in' >> ~/claude-wrapper && \
   echo '        --dangerously-enable-sudo|--dangerously-disable-firewall) ;;' >> ~/claude-wrapper && \
   echo '        *) FILTERED_ARGS+=("$arg") ;;' >> ~/claude-wrapper && \
   echo '    esac' >> ~/claude-wrapper && \
   echo 'done' >> ~/claude-wrapper && \
   echo '' >> ~/claude-wrapper && \
   echo 'exec claude "${FILTERED_ARGS[@]}"' >> ~/claude-wrapper && \
   chmod +x ~/claude-wrapper

WORKDIR /workspace

USER root
RUN echo '#!/bin/bash' > /usr/local/bin/docker-entrypoint && \
   echo 'ENABLE_SUDO=false' >> /usr/local/bin/docker-entrypoint && \
   echo 'DISABLE_FIREWALL=false' >> /usr/local/bin/docker-entrypoint && \
   echo 'for arg in "$@"; do' >> /usr/local/bin/docker-entrypoint && \
   echo '    case "$arg" in' >> /usr/local/bin/docker-entrypoint && \
   echo '        --dangerously-enable-sudo) ENABLE_SUDO=true ;;' >> /usr/local/bin/docker-entrypoint && \
   echo '        --dangerously-disable-firewall) DISABLE_FIREWALL=true ;;' >> /usr/local/bin/docker-entrypoint && \
   echo '    esac' >> /usr/local/bin/docker-entrypoint && \
   echo 'done' >> /usr/local/bin/docker-entrypoint && \
   echo '' >> /usr/local/bin/docker-entrypoint && \
   echo '# Run firewall setup AS ROOT before switching users' >> /usr/local/bin/docker-entrypoint && \
   echo 'export DISABLE_FIREWALL' >> /usr/local/bin/docker-entrypoint && \
   echo "if [ -f /home/$USERNAME/init-firewall.sh ]; then" >> /usr/local/bin/docker-entrypoint && \
   echo "    /home/$USERNAME/init-firewall.sh || true" >> /usr/local/bin/docker-entrypoint && \
   echo 'fi' >> /usr/local/bin/docker-entrypoint && \
   echo '' >> /usr/local/bin/docker-entrypoint && \
   echo '# NOW handle sudo setup if needed' >> /usr/local/bin/docker-entrypoint && \
   echo 'if [ "$ENABLE_SUDO" = "true" ]; then' >> /usr/local/bin/docker-entrypoint && \
   echo "    echo \"$USERNAME ALL=(ALL) NOPASSWD:ALL\" > /etc/sudoers.d/$USERNAME" >> /usr/local/bin/docker-entrypoint && \
   echo "    chmod 0440 /etc/sudoers.d/$USERNAME" >> /usr/local/bin/docker-entrypoint && \
   echo 'fi' >> /usr/local/bin/docker-entrypoint && \
   echo '' >> /usr/local/bin/docker-entrypoint && \
   echo '# Switch to user directory first' >> /usr/local/bin/docker-entrypoint && \
   echo "cd /home/$USERNAME" >> /usr/local/bin/docker-entrypoint && \
   echo '' >> /usr/local/bin/docker-entrypoint && \
   echo '# Finally switch to user and run command' >> /usr/local/bin/docker-entrypoint && \
   echo "exec su $USERNAME -c \"cd /workspace && /home/$USERNAME/claude-wrapper \\\"\\\$@\\\"\"" >> /usr/local/bin/docker-entrypoint && \
   chmod +x /usr/local/bin/docker-entrypoint
ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
EOF

progress_install() {
   local label="$1"
   local pct="$2"
   local bar_length=30
   local filled=$(( pct * bar_length / 100 ))
   local empty=$(( bar_length - filled ))
   local bar=$(printf "%0.s#" $(seq 1 $filled))
   local spaces=$(printf "%0.s " $(seq 1 $empty))
   printf "\r%-20s [%s%s] %3d%%" "$label" "$bar" "$spaces" "$pct"
   if [ "$pct" -eq 100 ]; then
       echo
   fi
}

   for i in {0..100..5}; do
       progress_install "Docker image" $i
       sleep 0.1
   done &
   PROGRESS_PID=$!

   docker build \
       --build-arg USER_ID=$USER_ID \
       --build-arg GROUP_ID=$GROUP_ID \
       --build-arg USERNAME=$DOCKER_USER \
       --build-arg NODE_VERSION="$NODE_VERSION" \
       -f "$DOCKERFILE" -t "$IMAGE_NAME" "$PROJECT_DIR" 2>&1 >/dev/null

   kill $PROGRESS_PID 2>/dev/null
   progress_install "Docker image" 100
   echo -e "\n${GREEN}Complete!${NC}\n"

   docker build \
       --build-arg USER_ID=$USER_ID \
       --build-arg GROUP_ID=$GROUP_ID \
       --build-arg USERNAME=$DOCKER_USER \
       --build-arg NODE_VERSION="$NODE_VERSION" \
       -f "$DOCKERFILE" -t "$IMAGE_NAME" "$PROJECT_DIR" 2>&1 | \
       grep -v 'invoke-rc.d' | grep -v 'policy-rc.d' || true

   rm -f "$DOCKERFILE"

   echo -e "${GREEN}Docker image '$IMAGE_NAME' built!${NC}"
   first_time=true
fi

# Install global symlink if needed (this is the only other place that might need sudo)
if [[ ! -L "$LINK_TARGET" ]]; then
   if [[ -w "$(dirname "$LINK_TARGET")" ]]; then
       # We can write to /usr/local/bin without sudo
       ln -s "$SCRIPT_PATH" "$LINK_TARGET"
       echo -e "${GREEN}'claudebox' is now globally available!${NC}"
   else
       # We need sudo for the symlink
       echo -e "${BLUE}Installing global symlink...${NC}"
       echo -e "${YELLOW}Creating symlink in /usr/local/bin requires sudo...${NC}"
       sudo ln -s "$SCRIPT_PATH" "$LINK_TARGET"
       echo -e "${GREEN}'claudebox' is now globally available!${NC}"
   fi
fi

# Show welcome message on first run
if [[ "$first_time" == true ]]; then
   echo
   echo -e "${CYAN}ClaudeBox Setup Complete!${NC}"
   echo
   echo -e "${GREEN}Quick Start:${NC}"
   echo -e "  ${YELLOW}claudebox [options]${NC}        # Launch Claude CLI"
   echo
   echo -e "${GREEN}Power Features:${NC}"
   echo -e "  ${YELLOW}claudebox profile${NC}                # See all language profiles"
   echo -e "  ${YELLOW}claudebox profile c openwrt${NC}      # Install C + OpenWRT tools"
   echo -e "  ${YELLOW}claudebox profile python ml${NC}      # Install Python + ML stack"
   echo -e "  ${YELLOW}claudebox install <packages>${NC}     # Install additional apt packages"
   echo -e "  ${YELLOW}claudebox shell${NC}                  # Open bash shell in container"
   echo
   echo -e "${GREEN}Security:${NC}"
   echo -e "  Network firewall: ON by default (Anthropic recommended)"
   echo -e "  Sudo access: OFF by default"
   echo
   echo -e "${PURPLE}Just install the profile you need and start coding!${NC}"
   exit 0
fi

docker run -it --rm \
   -w /workspace \
   -v "$PROJECT_DIR":/workspace \
   -v "$CLAUDE_DATA_DIR":/home/$DOCKER_USER/.claude \
   -v "$HOME/.claudebox":/home/$DOCKER_USER/.claudebox \
   -v "$HOME/.config/claude":/home/$DOCKER_USER/.config/claude \
   -v "$HOME/.claude.json":/home/$DOCKER_USER/.claude.json \
   -v "$HOME/.npmrc":/home/$DOCKER_USER/.npmrc:ro \
   -v "$HOME/.ssh":/home/$DOCKER_USER/.ssh:ro \
   -e "NODE_ENV=${NODE_ENV:-production}" \
   -e "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}" \
   --cap-add NET_ADMIN \
   --cap-add NET_RAW \
   "$IMAGE_NAME" "${USER_ARGS[@]}"
